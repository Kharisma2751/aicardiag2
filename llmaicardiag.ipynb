{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Installing collected packages: pyyaml\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed pyyaml-6.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chatterbot-corpus 1.2.0 requires PyYAML<4.0,>=3.12, but you have pyyaml 6.0.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 665/665 [00:00<?, ?B/s] \n",
      "model.safetensors: 100%|██████████| 548M/548M [02:22<00:00, 3.83MB/s] \n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<?, ?B/s] \n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 988kB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 3.74MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 5.04MB/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a problem with my car. my car brake has loose and I have to stop it. I am not sure if I can fix it or not.\n",
      "\n",
      "I am a car mechanic and have been working for a long time. My car is a little rusty and has a lot of loose parts. It is not a big problem. The only thing I would like to do is to fix the problem and get it fixed. If you have any questions, please feel free to contact me.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained GPT-2 model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Function to generate a response\n",
    "def generate_response(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=50)\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "user_input = input(\"Enter your car problem: \")\n",
    "prompt = \"I have a problem with my car. \" + user_input\n",
    "response = generate_response(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a problem with my car. iam a car user a chevrolet camaro 2014, the wiper cant working. the car is not working, i am not sure if it is a new or old car or if i have to replace it.\n",
      "\n",
      "I am a customer of a dealership and i can tell you that the dealership has a lot of problems with the vehicle. The dealership is very knowledgeable and they have been in touch with me for a long time. I have had\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_input = input(\"Enter your car problem: \")\n",
    "prompt = \"I have a problem with my car. \" + user_input\n",
    "response = generate_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initiate(user_input):\n",
    "  prompt = \"\"\"\n",
    "  add # or note above the code to explain the code line\n",
    "  \"\"\" + \"code =\",user_input\n",
    "\n",
    "  response = palm.chat(messages=user_input)\n",
    "  return response\n",
    "\n",
    "def reply(prev_msg, user_input):\n",
    "  return prev_msg.reply(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'last'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Kuliah\\Semester 7\\Big Dats\\LLM\\llmaicardiag.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Kuliah/Semester%207/Big%20Dats/LLM/llmaicardiag.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m response \u001b[39m=\u001b[39m initiate(\u001b[39minput\u001b[39m())\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Kuliah/Semester%207/Big%20Dats/LLM/llmaicardiag.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(response\u001b[39m.\u001b[39;49mlast)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'last'"
     ]
    }
   ],
   "source": [
    "response = initiate(input())\n",
    "print(response.last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initiate the conversation\n",
    "def initiate(user_input):\n",
    "    # Tokenize user input\n",
    "    input_ids = tokenizer.encode(user_input, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate response from the model\n",
    "    response_ids = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "    # Decode the generated response\n",
    "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply(prev_msg, user_input):\n",
    "    \"\"\"\n",
    "    Continue the conversation with the GPT-2 model.\n",
    "\n",
    "    Args:\n",
    "        prev_msg (str): The previous message or conversation history.\n",
    "        user_input (str): The user's input to continue the conversation.\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by the GPT-2 model.\n",
    "    \"\"\"\n",
    "    # Combine previous message and user input\n",
    "    conversation_history = prev_msg + \" \" + user_input\n",
    "\n",
    "    # Tokenize the combined conversation\n",
    "    input_ids = tokenizer.encode(conversation_history, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate response from the model\n",
    "    response_ids = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "    # Decode the generated response\n",
    "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my engine of 2014 buick can turn on and off at will.\n",
      "\n",
      "The engine is a 4.5 liter V8 engine with a displacement of 1,849 cc. The engine has a total displacement (4,908 cc) of 2,711 cc and a top speed of Mach 1.8. It has an output of 4,621 hp and an engine speed rating of 3,049 hp. This is the engine that will be used in the upcoming V\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "user_input = input(\"Enter your initial message: \")\n",
    "response = initiate(user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1281: UserWarning: Input length of input_ids is 105, but `max_length` is set to 100. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my engine of 2014 buick can turn on and off at will.\n",
      "\n",
      "The engine is a 4.5 liter V8 engine with a displacement of 1,849 cc. The engine has a total displacement (4,908 cc) of 2,711 cc and a top speed of Mach 1.8. It has an output of 4,621 hp and an engine speed rating of 3,049 hp. This is the engine that will be used in the upcoming V the engine still can starting from\n"
     ]
    }
   ],
   "source": [
    "# Continuing the conversation\n",
    "user_input = input(\"Enter your reply: \")\n",
    "response = reply(response, user_input)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
